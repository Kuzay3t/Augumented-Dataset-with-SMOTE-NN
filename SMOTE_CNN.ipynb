{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aX5UYLW6KOPo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from imblearn.over_sampling import SMOTE, SMOTENC\n",
        "from imblearn.combine import SMOTEENN  # SMOTE + Edited Nearest Neighbors\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# You can access the uploaded file(s) by iterating through the 'uploaded' dictionary\n",
        "# For example, to get the first uploaded file's name and content:\n",
        "# for fn, content in uploaded.items():\n",
        "#   print(f'User uploaded file \"{fn}\" with length {len(content)} bytes')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "U-I3KQQVKuaq",
        "outputId": "949fcd6e-3baf-49de-e6e0-43021554f4ba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cc132452-e0a5-4ea4-af6a-3ec6bd5fc8c1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cc132452-e0a5-4ea4-af6a-3ec6bd5fc8c1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Main Dataset.csv to Main Dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('Main Dataset.csv')\n"
      ],
      "metadata": {
        "id": "Bl6ara-kK2JM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original Dataset Info:\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZ1bPBKQK8lt",
        "outputId": "a1437374-37c2-4cf6-a7dd-b7a4df1b614f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Dataset Info:\n",
            "Shape: (30, 7)\n",
            "Columns: ['Grouo No', 'Leak Location 1', 'Leak Location 2', 'Inflow Rate (m³/s)', 'Pressure Drop (Pa)', 'Inflow velocity (m/s)', 'Outlet pressure (Pa)']\n",
            "\n",
            "First few rows:\n",
            "   Grouo No  Leak Location 1  Leak Location 2 Inflow Rate (m³/s)  \\\n",
            "0         1              0.4             0.75         3.13 ×10⁻⁴   \n",
            "1         2              0.4             0.75         3.23 ×10⁻⁴   \n",
            "2         3              0.4             0.75         3.66 ×10⁻⁴   \n",
            "3         4              0.6             0.86         3.21 ×10⁻⁴   \n",
            "4         5              0.6             0.86         3.44 ×10⁻⁴   \n",
            "\n",
            "  Pressure Drop (Pa)  Inflow velocity (m/s) Outlet pressure (Pa)  \n",
            "0              1,955                   1.26               15,340  \n",
            "1              2,028                   1.30               13,448  \n",
            "2              2,657                   1.47                5,367  \n",
            "3              2,112                   1.30               13,244  \n",
            "4              2,450                   1.39                9,612  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Clean the data - handle the scientific notation and commas\n",
        "def clean_numeric_data(df):\n",
        "    \"\"\"Clean numeric columns with scientific notation and commas\"\"\"\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    # Clean column names\n",
        "    df_clean.columns = df_clean.columns.str.strip()\n",
        "\n",
        "    # Convert scientific notation in Inflow Rate\n",
        "    if 'Inflow Rate (m³/s)' in df_clean.columns:\n",
        "        # Handle both '×' and spaces in scientific notation\n",
        "        df_clean['Inflow Rate (m³/s)'] = (df_clean['Inflow Rate (m³/s)']\n",
        "                                         .astype(str)\n",
        "                                         .str.replace('×', 'e')\n",
        "                                         .str.replace(' e', 'e')  # Remove space before e\n",
        "                                         .str.replace('e-', 'e-'))  # Ensure proper e- format\n",
        "        df_clean['Inflow Rate (m³/s)'] = pd.to_numeric(df_clean['Inflow Rate (m³/s)'], errors='coerce')\n",
        "\n",
        "    # Remove commas from numeric columns and convert to float\n",
        "    numeric_columns = ['Pressure Drop (Pa)', 'Outlet pressure (Pa)']\n",
        "    for col in numeric_columns:\n",
        "        if col in df_clean.columns:\n",
        "            df_clean[col] = (df_clean[col]\n",
        "                           .astype(str)\n",
        "                           .str.replace(',', '')\n",
        "                           .str.replace('\"', ''))  # Remove any quotes\n",
        "            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
        "\n",
        "    # Also clean other numeric columns\n",
        "    other_numeric = ['Inflow velocity (m/s)']\n",
        "    for col in other_numeric:\n",
        "        if col in df_clean.columns:\n",
        "            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
        "\n",
        "    return df_clean"
      ],
      "metadata": {
        "id": "URlfK_bAK-WB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean the dataset\n",
        "df_clean = clean_numeric_data(df)\n"
      ],
      "metadata": {
        "id": "D6cLyGocNcSC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle any remaining NaN values\n",
        "if df_clean.isnull().any().any():\n",
        "    print(\"\\n⚠️  Found NaN values. Handling them...\")\n",
        "\n",
        "    # For numeric columns, fill NaN with median\n",
        "    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
        "    for col in numeric_cols:\n",
        "        if df_clean[col].isnull().any():\n",
        "            median_val = df_clean[col].median()\n",
        "            df_clean[col].fillna(median_val, inplace=True)\n",
        "            print(f\"  Filled NaN in '{col}' with median: {median_val}\")\n",
        "\n",
        "print(\"\\nFinal data check:\")\n",
        "print(\"Missing values after cleaning:\", df_clean.isnull().sum().sum())\n",
        "print(\"Data shape:\", df_clean.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxVoreUhNg8v",
        "outputId": "8f86e327-278e-4acb-fe75-d44338dc810b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "⚠️  Found NaN values. Handling them...\n",
            "  Filled NaN in 'Inflow Rate (m³/s)' with median: nan\n",
            "\n",
            "Final data check:\n",
            "Missing values after cleaning: 30\n",
            "Data shape: (30, 7)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1365636616.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_clean[col].fillna(median_val, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages first:\n",
        "# pip install imbalanced-learn scikit-learn numpy pandas\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from imblearn.over_sampling import SMOTE, SMOTENC\n",
        "from imblearn.combine import SMOTEENN  # SMOTE + Edited Nearest Neighbors\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv('Main Dataset.csv')\n",
        "\n",
        "print(\"Original Dataset Info:\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# Clean the data - handle the scientific notation and commas\n",
        "def clean_numeric_data(df):\n",
        "    \"\"\"Clean numeric columns with scientific notation and commas\"\"\"\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    # Clean column names\n",
        "    df_clean.columns = df_clean.columns.str.strip()\n",
        "\n",
        "    # Debug: Print the original inflow rate values\n",
        "    if 'Inflow Rate (m³/s)' in df_clean.columns:\n",
        "        print(\"Original inflow rate values (first 5):\")\n",
        "        print(df_clean['Inflow Rate (m³/s)'].head().values)\n",
        "\n",
        "        # Convert scientific notation in Inflow Rate - step by step\n",
        "        inflow_col = df_clean['Inflow Rate (m³/s)'].astype(str)\n",
        "        print(\"\\nAfter converting to string:\")\n",
        "        print(inflow_col.head().values)\n",
        "\n",
        "        # Replace the scientific notation symbols - handle Unicode superscripts\n",
        "        inflow_col = inflow_col.str.replace('×10⁻', 'e-')  # Handle ×10⁻ format\n",
        "        inflow_col = inflow_col.str.replace('×', 'e')      # Handle × format\n",
        "        inflow_col = inflow_col.str.replace(' e', 'e')     # Remove space before e\n",
        "\n",
        "        # Handle Unicode superscript numbers\n",
        "        superscript_map = {\n",
        "            '⁰': '0', '¹': '1', '²': '2', '³': '3', '⁴': '4',\n",
        "            '⁵': '5', '⁶': '6', '⁷': '7', '⁸': '8', '⁹': '9'\n",
        "        }\n",
        "        for super_char, regular_char in superscript_map.items():\n",
        "            inflow_col = inflow_col.str.replace(super_char, regular_char)\n",
        "\n",
        "        print(\"\\nAfter replacing scientific notation and superscripts:\")\n",
        "        print(inflow_col.head().values)\n",
        "\n",
        "        # Convert to numeric\n",
        "        df_clean['Inflow Rate (m³/s)'] = pd.to_numeric(inflow_col, errors='coerce')\n",
        "        print(\"\\nAfter converting to numeric:\")\n",
        "        print(df_clean['Inflow Rate (m³/s)'].head().values)\n",
        "\n",
        "    # Remove commas from numeric columns and convert to float\n",
        "    numeric_columns = ['Pressure Drop (Pa)', 'Outlet pressure (Pa)']\n",
        "    for col in numeric_columns:\n",
        "        if col in df_clean.columns:\n",
        "            df_clean[col] = (df_clean[col]\n",
        "                           .astype(str)\n",
        "                           .str.replace(',', '')\n",
        "                           .str.replace('\"', ''))  # Remove any quotes\n",
        "            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
        "\n",
        "    # Also clean other numeric columns\n",
        "    other_numeric = ['Inflow velocity (m/s)']\n",
        "    for col in other_numeric:\n",
        "        if col in df_clean.columns:\n",
        "            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "# Clean the dataset\n",
        "df_clean = clean_numeric_data(df)\n",
        "\n",
        "# Check for missing values and handle them\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Cleaned Dataset:\")\n",
        "print(df_clean.dtypes)\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df_clean.isnull().sum())\n",
        "print(\"\\nCleaned data sample:\")\n",
        "print(df_clean.head())\n",
        "\n",
        "# Handle any remaining NaN values\n",
        "if df_clean.isnull().any().any():\n",
        "    print(\"\\n⚠️  Found NaN values. Handling them...\")\n",
        "\n",
        "    # For numeric columns, fill NaN with median (but only if median is not NaN)\n",
        "    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
        "    for col in numeric_cols:\n",
        "        if df_clean[col].isnull().any():\n",
        "            median_val = df_clean[col].median()\n",
        "            if not pd.isna(median_val):\n",
        "                df_clean.loc[:, col] = df_clean[col].fillna(median_val)\n",
        "                print(f\"  Filled NaN in '{col}' with median: {median_val}\")\n",
        "            else:\n",
        "                print(f\"  ⚠️  Cannot fill '{col}' - all values are NaN!\")\n",
        "                # If all values are NaN, there's a data cleaning issue\n",
        "                print(f\"     Original values in {col}:\")\n",
        "                print(f\"     {df[col].head().values}\")\n",
        "\n",
        "print(\"\\nFinal data check:\")\n",
        "print(\"Missing values after cleaning:\", df_clean.isnull().sum().sum())\n",
        "print(\"Data shape:\", df_clean.shape)\n",
        "\n",
        "# Prepare features and target\n",
        "# For this example, let's create categories based on pressure ranges as target\n",
        "# You can modify this based on your specific prediction task\n",
        "\n",
        "# Method 1: Create pressure categories as target variable\n",
        "df_clean['Pressure_Category'] = pd.cut(\n",
        "    df_clean['Outlet pressure (Pa)'],\n",
        "    bins=3,\n",
        "    labels=['Low', 'Medium', 'High']\n",
        ")\n",
        "\n",
        "# Prepare feature matrix (X) and target vector (y)\n",
        "feature_columns = [\n",
        "    'Leak Location 1', 'Leak Location 2', 'Inflow Rate (m³/s)',\n",
        "    'Pressure Drop (Pa)', 'Inflow velocity (m/s)'\n",
        "]\n",
        "\n",
        "X = df_clean[feature_columns].values\n",
        "y = df_clean['Pressure_Category']\n",
        "\n",
        "# Check for any remaining NaN values in features or target\n",
        "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
        "print(f\"NaN values in features: {np.isnan(X).sum()}\")\n",
        "print(f\"NaN values in target: {y.isnull().sum()}\")\n",
        "\n",
        "# Remove any rows with NaN values if they still exist\n",
        "if np.isnan(X).any() or y.isnull().any():\n",
        "    print(\"⚠️  Removing rows with NaN values...\")\n",
        "    mask = ~(np.isnan(X).any(axis=1) | y.isnull())\n",
        "    X = X[mask]\n",
        "    y = y[mask]\n",
        "    print(f\"Final data shape after removing NaN: {X.shape}\")\n",
        "\n",
        "# Encode target labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "print(f\"\\nOriginal class distribution: {Counter(y_encoded)}\")\n",
        "print(f\"Class labels: {dict(zip(le.classes_, range(len(le.classes_))))}\")\n",
        "\n",
        "# Validate we have enough samples for SMOTE\n",
        "min_class_size = min(Counter(y_encoded).values())\n",
        "if min_class_size < 2:\n",
        "    raise ValueError(f\"Minimum class size is {min_class_size}. Need at least 2 samples per class for SMOTE.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# SMOTE Implementation for your dataset\n",
        "\n",
        "# 1. Standard SMOTE\n",
        "print(\"1. APPLYING STANDARD SMOTE\")\n",
        "\n",
        "# For small datasets, use smaller k_neighbors\n",
        "min_class_size = min(Counter(y_encoded).values())\n",
        "k_neighbors = min(3, min_class_size - 1) if min_class_size > 1 else 1\n",
        "\n",
        "smote = SMOTE(\n",
        "    sampling_strategy='auto',  # Balance all classes\n",
        "    k_neighbors=k_neighbors,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_smote, y_smote = smote.fit_resample(X, y_encoded)\n",
        "\n",
        "print(f\"After SMOTE:\")\n",
        "print(f\"  Original shape: {X.shape}\")\n",
        "print(f\"  Augmented shape: {X_smote.shape}\")\n",
        "print(f\"  Original distribution: {Counter(y_encoded)}\")\n",
        "print(f\"  Augmented distribution: {Counter(y_smote)}\")\n",
        "\n",
        "# 2. SMOTE with Edited Nearest Neighbors (SMOTE-ENN)\n",
        "print(\"\\n2. APPLYING SMOTE + EDITED NEAREST NEIGHBORS\")\n",
        "\n",
        "smote_enn = SMOTEENN(\n",
        "    smote=SMOTE(k_neighbors=k_neighbors, random_state=42),\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_smote_enn, y_smote_enn = smote_enn.fit_resample(X, y_encoded)\n",
        "\n",
        "print(f\"After SMOTE-ENN:\")\n",
        "print(f\"  Shape: {X_smote_enn.shape}\")\n",
        "print(f\"  Distribution: {Counter(y_smote_enn)}\")\n",
        "\n",
        "# 3. Custom SMOTE with specific augmentation\n",
        "print(\"\\n3. CUSTOM SMOTE FOR SPECIFIC AUGMENTATION\")\n",
        "\n",
        "# Let's say you want 100 samples total\n",
        "target_samples_per_class = 35  # This will give you ~105 total samples\n",
        "\n",
        "smote_custom = SMOTE(\n",
        "    sampling_strategy={i: target_samples_per_class for i in range(len(le.classes_))},\n",
        "    k_neighbors=k_neighbors,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_smote_custom, y_smote_custom = smote_custom.fit_resample(X, y_encoded)\n",
        "\n",
        "print(f\"After Custom SMOTE:\")\n",
        "print(f\"  Shape: {X_smote_custom.shape}\")\n",
        "print(f\"  Distribution: {Counter(y_smote_custom)}\")\n",
        "\n",
        "# Convert back to DataFrame for easier analysis\n",
        "def create_augmented_dataframe(X_aug, y_aug, feature_columns, label_encoder):\n",
        "    \"\"\"Convert augmented arrays back to DataFrame\"\"\"\n",
        "    df_aug = pd.DataFrame(X_aug, columns=feature_columns)\n",
        "    df_aug['Pressure_Category'] = label_encoder.inverse_transform(y_aug)\n",
        "    df_aug['Group_No'] = range(1, len(df_aug) + 1)  # New group numbers\n",
        "\n",
        "    # Reorder columns to match original\n",
        "    column_order = ['Group_No'] + feature_columns + ['Pressure_Category']\n",
        "    df_aug = df_aug[column_order]\n",
        "\n",
        "    return df_aug\n",
        "\n",
        "# Create augmented datasets\n",
        "df_smote = create_augmented_dataframe(X_smote, y_smote, feature_columns, le)\n",
        "df_smote_custom = create_augmented_dataframe(X_smote_custom, y_smote_custom, feature_columns, le)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"AUGMENTED DATASET SAMPLES:\")\n",
        "print(\"\\nFirst 10 rows of SMOTE augmented dataset:\")\n",
        "print(df_smote.head(10))\n",
        "\n",
        "# Save the augmented datasets\n",
        "df_smote.to_csv('Dataset_SMOTE_Augmented.csv', index=False)\n",
        "df_smote_custom.to_csv('Dataset_SMOTE_Custom_Augmented.csv', index=False)\n",
        "\n",
        "print(f\"\\n✅ Saved augmented datasets:\")\n",
        "print(f\"   - Dataset_SMOTE_Augmented.csv ({len(df_smote)} samples)\")\n",
        "print(f\"   - Dataset_SMOTE_Custom_Augmented.csv ({len(df_smote_custom)} samples)\")\n",
        "\n",
        "# Alternative approach: Regression-based augmentation\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"4. ALTERNATIVE: CONTINUOUS TARGET APPROACH\")\n",
        "print(\"If you want to predict a continuous variable (like outlet pressure):\")\n",
        "\n",
        "# Use outlet pressure as continuous target\n",
        "X_features = df_clean[feature_columns[:-1]].values  # Exclude outlet pressure from features\n",
        "y_continuous = df_clean['Outlet pressure (Pa)'].values\n",
        "\n",
        "# Create discrete bins for SMOTE\n",
        "y_binned = pd.cut(y_continuous, bins=5, labels=False)\n",
        "\n",
        "smote_continuous = SMOTE(\n",
        "    sampling_strategy='auto',\n",
        "    k_neighbors=min(2, min(Counter(y_binned).values()) - 1),\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_cont_smote, y_cont_smote = smote_continuous.fit_resample(X_features, y_binned)\n",
        "\n",
        "print(f\"Continuous approach - Shape: {X_cont_smote.shape}\")\n",
        "print(f\"Continuous approach - Bin distribution: {Counter(y_cont_smote)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RECOMMENDATIONS FOR YOUR DATASET:\")\n",
        "print(\"1. Use k_neighbors=2 or 3 for small datasets (30 samples)\")\n",
        "print(\"2. Consider the physical meaning of synthetic samples\")\n",
        "print(\"3. Validate synthetic samples make physical sense\")\n",
        "print(\"4. Use cross-validation to assess model performance\")\n",
        "print(\"5. Consider domain-specific constraints when generating synthetic data\")\n",
        "\n",
        "# Function for easy reuse\n",
        "def smote_augment_fluid_data(csv_file, target_column_type='pressure_categories', n_samples=100):\n",
        "    \"\"\"\n",
        "    Easy function to apply SMOTE to your fluid dynamics data\n",
        "\n",
        "    Parameters:\n",
        "    csv_file: path to your CSV file\n",
        "    target_column_type: 'pressure_categories' or 'custom'\n",
        "    n_samples: target number of total samples\n",
        "    \"\"\"\n",
        "\n",
        "    df = pd.read_csv(csv_file)\n",
        "    df_clean = clean_numeric_data(df)\n",
        "\n",
        "    # Define features\n",
        "    feature_cols = [\n",
        "        'Leak Location 1', 'Leak Location 2', 'Inflow Rate (m³/s)',\n",
        "        'Pressure Drop (Pa)', 'Inflow velocity (m/s)'\n",
        "    ]\n",
        "\n",
        "    X = df_clean[feature_cols].values\n",
        "\n",
        "    if target_column_type == 'pressure_categories':\n",
        "        y = pd.cut(df_clean['Outlet pressure (Pa)'], bins=3, labels=False)\n",
        "\n",
        "    # Apply SMOTE\n",
        "    smote = SMOTE(\n",
        "        sampling_strategy='auto',\n",
        "        k_neighbors=min(3, len(df_clean) // 4),\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    X_aug, y_aug = smote.fit_resample(X, y)\n",
        "\n",
        "    return X_aug, y_aug\n",
        "\n",
        "print(\"\\n✅ Complete! Check the generated CSV files for your augmented datasets.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQPTshuSLDwz",
        "outputId": "c4715233-746f-440e-9ed8-69b8e3931351"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Dataset Info:\n",
            "Shape: (30, 7)\n",
            "Columns: ['Grouo No', 'Leak Location 1', 'Leak Location 2', 'Inflow Rate (m³/s)', 'Pressure Drop (Pa)', 'Inflow velocity (m/s)', 'Outlet pressure (Pa)']\n",
            "\n",
            "First few rows:\n",
            "   Grouo No  Leak Location 1  Leak Location 2 Inflow Rate (m³/s)  \\\n",
            "0         1              0.4             0.75         3.13 ×10⁻⁴   \n",
            "1         2              0.4             0.75         3.23 ×10⁻⁴   \n",
            "2         3              0.4             0.75         3.66 ×10⁻⁴   \n",
            "3         4              0.6             0.86         3.21 ×10⁻⁴   \n",
            "4         5              0.6             0.86         3.44 ×10⁻⁴   \n",
            "\n",
            "  Pressure Drop (Pa)  Inflow velocity (m/s) Outlet pressure (Pa)  \n",
            "0              1,955                   1.26               15,340  \n",
            "1              2,028                   1.30               13,448  \n",
            "2              2,657                   1.47                5,367  \n",
            "3              2,112                   1.30               13,244  \n",
            "4              2,450                   1.39                9,612  \n",
            "Original inflow rate values (first 5):\n",
            "['3.13 ×10⁻⁴' '3.23 ×10⁻⁴' '3.66 ×10⁻⁴' '3.21 ×10⁻⁴' '3.44 ×10⁻⁴']\n",
            "\n",
            "After converting to string:\n",
            "['3.13 ×10⁻⁴' '3.23 ×10⁻⁴' '3.66 ×10⁻⁴' '3.21 ×10⁻⁴' '3.44 ×10⁻⁴']\n",
            "\n",
            "After replacing scientific notation and superscripts:\n",
            "['3.13e-4' '3.23e-4' '3.66e-4' '3.21e-4' '3.44e-4']\n",
            "\n",
            "After converting to numeric:\n",
            "[0.000313 0.000323 0.000366 0.000321 0.000344]\n",
            "\n",
            "============================================================\n",
            "Cleaned Dataset:\n",
            "Grouo No                   int64\n",
            "Leak Location 1          float64\n",
            "Leak Location 2          float64\n",
            "Inflow Rate (m³/s)       float64\n",
            "Pressure Drop (Pa)         int64\n",
            "Inflow velocity (m/s)    float64\n",
            "Outlet pressure (Pa)       int64\n",
            "dtype: object\n",
            "\n",
            "Missing values per column:\n",
            "Grouo No                 0\n",
            "Leak Location 1          0\n",
            "Leak Location 2          0\n",
            "Inflow Rate (m³/s)       0\n",
            "Pressure Drop (Pa)       0\n",
            "Inflow velocity (m/s)    0\n",
            "Outlet pressure (Pa)     0\n",
            "dtype: int64\n",
            "\n",
            "Cleaned data sample:\n",
            "   Grouo No  Leak Location 1  Leak Location 2  Inflow Rate (m³/s)  \\\n",
            "0         1              0.4             0.75            0.000313   \n",
            "1         2              0.4             0.75            0.000323   \n",
            "2         3              0.4             0.75            0.000366   \n",
            "3         4              0.6             0.86            0.000321   \n",
            "4         5              0.6             0.86            0.000344   \n",
            "\n",
            "   Pressure Drop (Pa)  Inflow velocity (m/s)  Outlet pressure (Pa)  \n",
            "0                1955                   1.26                 15340  \n",
            "1                2028                   1.30                 13448  \n",
            "2                2657                   1.47                  5367  \n",
            "3                2112                   1.30                 13244  \n",
            "4                2450                   1.39                  9612  \n",
            "\n",
            "Final data check:\n",
            "Missing values after cleaning: 0\n",
            "Data shape: (30, 7)\n",
            "\n",
            "Feature matrix shape: (30, 5)\n",
            "NaN values in features: 0\n",
            "NaN values in target: 0\n",
            "\n",
            "Original class distribution: Counter({np.int64(0): 12, np.int64(1): 10, np.int64(2): 8})\n",
            "Class labels: {'High': 0, 'Low': 1, 'Medium': 2}\n",
            "\n",
            "============================================================\n",
            "1. APPLYING STANDARD SMOTE\n",
            "After SMOTE:\n",
            "  Original shape: (30, 5)\n",
            "  Augmented shape: (36, 5)\n",
            "  Original distribution: Counter({np.int64(0): 12, np.int64(1): 10, np.int64(2): 8})\n",
            "  Augmented distribution: Counter({np.int64(0): 12, np.int64(1): 12, np.int64(2): 12})\n",
            "\n",
            "2. APPLYING SMOTE + EDITED NEAREST NEIGHBORS\n",
            "After SMOTE-ENN:\n",
            "  Shape: (30, 5)\n",
            "  Distribution: Counter({np.int64(0): 11, np.int64(1): 10, np.int64(2): 9})\n",
            "\n",
            "3. CUSTOM SMOTE FOR SPECIFIC AUGMENTATION\n",
            "After Custom SMOTE:\n",
            "  Shape: (105, 5)\n",
            "  Distribution: Counter({np.int64(0): 35, np.int64(1): 35, np.int64(2): 35})\n",
            "\n",
            "============================================================\n",
            "AUGMENTED DATASET SAMPLES:\n",
            "\n",
            "First 10 rows of SMOTE augmented dataset:\n",
            "   Group_No  Leak Location 1  Leak Location 2  Inflow Rate (m³/s)  \\\n",
            "0         1             0.40             0.75            0.000313   \n",
            "1         2             0.40             0.75            0.000323   \n",
            "2         3             0.40             0.75            0.000366   \n",
            "3         4             0.60             0.86            0.000321   \n",
            "4         5             0.60             0.86            0.000344   \n",
            "5         6             0.60             0.86            0.000358   \n",
            "6         7             0.14             0.60            0.000320   \n",
            "7         8             0.14             0.60            0.000352   \n",
            "8         9             0.14             0.60            0.000370   \n",
            "9        10             0.14             0.39            0.000325   \n",
            "\n",
            "   Pressure Drop (Pa)  Inflow velocity (m/s) Pressure_Category  \n",
            "0              1955.0                   1.26              High  \n",
            "1              2028.0                   1.30              High  \n",
            "2              2657.0                   1.47               Low  \n",
            "3              2112.0                   1.30              High  \n",
            "4              2450.0                   1.39            Medium  \n",
            "5              2509.0                   1.44               Low  \n",
            "6              2141.0                   1.29              High  \n",
            "7              2571.0                   1.42            Medium  \n",
            "8              2770.0                   1.49               Low  \n",
            "9              2176.0                   1.31              High  \n",
            "\n",
            "✅ Saved augmented datasets:\n",
            "   - Dataset_SMOTE_Augmented.csv (36 samples)\n",
            "   - Dataset_SMOTE_Custom_Augmented.csv (105 samples)\n",
            "\n",
            "============================================================\n",
            "4. ALTERNATIVE: CONTINUOUS TARGET APPROACH\n",
            "If you want to predict a continuous variable (like outlet pressure):\n",
            "Continuous approach - Shape: (50, 4)\n",
            "Continuous approach - Bin distribution: Counter({np.int64(4): 10, np.int64(0): 10, np.int64(3): 10, np.int64(2): 10, np.int64(1): 10})\n",
            "\n",
            "============================================================\n",
            "RECOMMENDATIONS FOR YOUR DATASET:\n",
            "1. Use k_neighbors=2 or 3 for small datasets (30 samples)\n",
            "2. Consider the physical meaning of synthetic samples\n",
            "3. Validate synthetic samples make physical sense\n",
            "4. Use cross-validation to assess model performance\n",
            "5. Consider domain-specific constraints when generating synthetic data\n",
            "\n",
            "✅ Complete! Check the generated CSV files for your augmented datasets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"Current working directory:\", os.getcwd())"
      ],
      "metadata": {
        "id": "WpHbyzxnM6hV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12814af0-1426-41f4-acf9-43b78291c71f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6VY-fmCEEHt4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}